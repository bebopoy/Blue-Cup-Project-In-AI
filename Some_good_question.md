# 卷积-尺寸变化

好的！我们来逐步解释 `self.fc1 = nn.Linear(16 * 6 * 6, 120)` 这行代码的含义，特别是为什么输入维度是 `16 * 6 * 6`。

---

### 1. **`nn.Linear` 的作用**

`nn.Linear` 是 PyTorch 中的一个全连接层（也称为线性层或仿射层）。它的作用是将输入数据进行线性变换，公式为：
\[ y = Wx + b \]
其中：

- \( x \) 是输入数据（一个向量），
- \( W \) 是权重矩阵，
- \( b \) 是偏置向量，
- \( y \) 是输出数据。

`nn.Linear` 需要指定两个参数：

- **输入维度（`in_features`）**：输入向量的长度。
- **输出维度（`out_features`）**：输出向量的长度。

在 `self.fc1 = nn.Linear(16 * 6 * 6, 120)` 中：

- 输入维度是 `16 * 6 * 6`，
- 输出维度是 `120`。

---

### 2. **为什么输入维度是 `16 * 6 * 6`？**

输入维度 `16 * 6 * 6` 是由前面的卷积层和池化层的输出决定的。我们一步一步来看：

#### （1）输入数据

假设输入是一个单通道的图像，大小为 `1 x 32 x 32`（即 1 个通道，高度 32，宽度 32）。

#### （2）第一层卷积 `self.conv1`

- 卷积核大小：`3x3`，
- 输入通道数：`1`，
- 输出通道数：`6`。

卷积操作后，输出的特征图大小为：
\[ \text{输出大小} = \frac{\text{输入大小} - \text{卷积核大小} + 2 \times \text{填充}}{\text{步长}} + 1 \]
默认情况下，填充为 0，步长为 1，因此：
\[ \text{输出大小} = \frac{32 - 3 + 0}{1} + 1 = 30 \]
所以，`self.conv1` 的输出是 `6 x 30 x 30`（6 个通道，高度 30，宽度 30）。

#### （3）第一层池化 `F.max_pool2d`

- 池化窗口大小：`2x2`，
- 步长：`2`。

池化操作后，输出的特征图大小为：
\[ \text{输出大小} = \frac{\text{输入大小}}{\text{池化窗口大小}} \]
因此：
\[ \text{输出大小} = \frac{30}{2} = 15 \]
所以，池化后的输出是 `6 x 15 x 15`。

#### （4）第二层卷积 `self.conv2`

- 卷积核大小：`3x3`，
- 输入通道数：`6`，
- 输出通道数：`16`。

卷积操作后，输出的特征图大小为：
\[ \text{输出大小} = \frac{15 - 3 + 0}{1} + 1 = 13 \]
所以，`self.conv2` 的输出是 `16 x 13 x 13`。

#### （5）第二层池化 `F.max_pool2d`

- 池化窗口大小：`2x2`，
- 步长：`2`。

池化操作后，输出的特征图大小为：
\[ \text{输出大小} = \frac{13}{2} = 6.5 \]
由于池化操作会向下取整，因此输出大小是 `6`。
所以，池化后的输出是 `16 x 6 x 6`。

#### （6）展平操作 `x.view(-1, self.num_flat_features(x))`

在进入全连接层之前，需要将多维的特征图展平成一维向量。`16 x 6 x 6` 的特征图展平后的长度为：
\[ 16 \times 6 \times 6 = 576 \]
因此，全连接层 `self.fc1` 的输入维度是 `576`，即 `16 * 6 * 6`。

---

### 3. **总结**

- `16 * 6 * 6` 是经过卷积和池化操作后，特征图的形状（`16` 个通道，每个通道大小为 `6 x 6`）。
- 全连接层 `self.fc1` 的输入维度需要与展平后的特征图长度一致，因此是 `16 * 6 * 6`。
- 输出维度 `120` 是人为设定的，可以根据任务需求调整。

希望这个解释能帮助你理解！如果还有疑问，欢迎继续提问！

## 池化操作

https://blog.csdn.net/xholes/article/details/78491175

如果说卷积是在挖掘特征，那么池化就是在压缩特征。

而且池化是一个非常暴力的操作，它是直接把特征图的尺寸压缩，所以会丢失一些信息，但有效！

池化操作的作用是减小特征图的尺寸，减少计算量，同时保留主要的特征信息。

池化操作通常有两种类型：最大池化（Max Pooling）和平均池化（Average Pooling）。
