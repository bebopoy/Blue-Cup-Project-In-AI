{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型部署 使用flask python \n",
    "网址： \n",
    "https://www.lanqiao.cn/courses/40981/learning/?id=2813919&compatibility=false\n",
    "\n",
    "# 原题\n",
    "```python \n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "def generate_text(start_sequence, max_length):\n",
    "    #TODO\n",
    "\n",
    "\n",
    "    pass\n",
    "\n",
    "@app.route('/search', methods=['POST'])\n",
    "def search():\n",
    "    #TODO\n",
    "    \n",
    "    \n",
    "    return jsonify(text=generated_text) # generated_text 是 generate_text 函数返回值\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "```\n",
    "---\n",
    "# 手打题目\n",
    "```python \n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "def generate_text(start_sequence, max_length):\n",
    "    # 加载训练好的语言模型\n",
    "    model = load_model(\"lm.h5\")\n",
    "    # 加载词汇表和索引转换字典\n",
    "    with open(\"vocab.json\", 'r', encoding='utf-8') as f:\n",
    "        vocab_data = json.load(f)\n",
    "        word_to_idx = vocab_data['word_to_idx']\n",
    "        idx_to_word = vocab_data['idx_to_word']\n",
    "\n",
    "    generated_text = start_sequence\n",
    "    input_seq = [word_to_idx[word] for word in generated_text.split()]\n",
    "    input_seq = np.array(input_seq).reshape(1, -1)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        predictions = model.predict(input_seq)\n",
    "        pred_idx = np.argmax(predictions)\n",
    "        word = idx_to_word[str(pred_idx)]\n",
    "        generated_text += \" \" + word\n",
    "        input_seq = np.append(input_seq, pred_idx)\n",
    "        input_seq = input_seq[1:].reshape(1, -1)\n",
    "\n",
    "    generated_indexes = [word_to_idx[word] for word in generated_text.split()]\n",
    "    return generated_indexes\n",
    "\n",
    "@app.route('/search', methods=['POST'])\n",
    "def search():\n",
    "    data = request.get_json()\n",
    "    start_sequence = data['start_sequence']\n",
    "    max_length = data['max_length']  # 从请求的 JSON 数据中获取 max_length 值\n",
    "    generated_text = generate_text(start_sequence, max_length)\n",
    "    return jsonify(text=generated_text)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "\n",
    "\n",
    "```\n",
    "<mark>ERROR</mark>: \n",
    "* 从请求体中取出目标变量值 data = request.get_json() 使用 start_sequence = data['start_sequence'] 注意引号不可省略\n",
    "* with open(\"vocab.json\", 'r', encoding='utf-8') as f:\n",
    "        vocab_data = json.load(f)\n",
    "        word_to_idx = vocab_data['word_to_idx']\n",
    "        idx_to_word = vocab_data['idx_to_word']\n",
    "  读取一个json 文件，先打开后as file ，使用 json.load(file) ,同样的需要在使用时添加['str']\n",
    "* 模型的输入是一个 np narray 类型 (input_seq 由查询得来) ，输出 是一个narray,在其中找到最大的对应的序号， predix = np.argmax(predictions), 对应的单词为 idx_to_word[str(predix)] 仅需将最初的输入与后续输出的单词 字符串拼接在一起即可 ，但是对于下次的输入 将改为 input_seq = np.append(input_seq, predix) 具体的修改，可能是去除历史第一个或第二个 input_seq = input_seq[2:].reshape(1,-1) \n",
    "---\n",
    "# 参考答案\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型部署 使用flask python \n",
    "\n",
    "介绍\n",
    "在本任务中，你将使用训练好的语言模型作为底层模型之一来实现辅助用户搜索。一种实现方式是将语言模型封装为一个 Web API，提供对外的接口服务。通过定义 API 路由和请求处理逻辑，接收用户的输入请求并返回结果。\n",
    "\n",
    "准备\n",
    "开始答题前，请确认 /home/project 目录下包含以下文件：\n",
    "\n",
    "lm.h5，是本任务提供的语言模型。你可以使用下面的代码查看模型。\n",
    "```\n",
    "import tensorflow\n",
    "model = tensorflow.keras.models.load_model(\"lm.h5\")\n",
    "model.summary()\n",
    "```\n",
    "vocab.json，包含了词汇表和索引转换字典，可用于将文本中的词汇转换为语言模型能够理解的索引表示，形如：\n",
    "```\n",
    "{\n",
    "  \"word_to_idx\": {\n",
    "    \"apple\": 0,\n",
    "    \"banana\": 1,\n",
    "    \"orange\": 2,\n",
    "    \"grape\": 3,\n",
    "    \"pear\": 4\n",
    "  },\n",
    "  \"idx_to_word\": {\n",
    "    \"0\": \"apple\",\n",
    "    \"1\": \"banana\",\n",
    "    \"2\": \"orange\",\n",
    "    \"3\": \"grape\",\n",
    "    \"4\": \"pear\"\n",
    "  }\n",
    "} \n",
    "```\n",
    "task.py，是你后续答题过程中编写代码的地方。\n",
    "\n",
    "目标\n",
    "请按要求实现以下目标。\n",
    "\n",
    "目标 1：实现 generate_text 函数\n",
    "\n",
    "参数\n",
    "start_sequence（字符串）：文本的起始词。\n",
    "max_length（整数）：模型生成词的最大个数。\n",
    "功能\n",
    "加载提供的语言模型和词汇表。\n",
    "使用语言模型生成文本，最多生成 max_length 个词。\n",
    "返回值\n",
    "generated_indexes（列表）：最终文本的索引表示，即每个词对应的索引（含起始词索引）。\n",
    "目标 2：实现 search 函数\n",
    "\n",
    "参数\n",
    "无。\n",
    "功能\n",
    "处理 POST 请求，接收客户端发送的 JSON 数据，提取起始词（start_sequence）和最大词长（max_length）。\n",
    "调用 generate_text 函数生成文本，以 JSON 格式响应给客户端。\n",
    "返回值\n",
    "JSON 数据，包含名为 text 的字段，其值为生成的文本的索引列表。\n",
    "请求示例：\n",
    "```\n",
    "curl -X POST -H \"Content-Type: application/json\" -d '{\"start_sequence\": \"生活\", \"max_length\": 5}' http://localhost:5000/search\n",
    "```\n",
    "输出示例：\n",
    "```\n",
    "{\n",
    "  \"text\": [\n",
    "    12139,\n",
    "    123,\n",
    "    367,\n",
    "    1200,\n",
    "    5678,\n",
    "    333\n",
    "  ]\n",
    "}\n",
    "```\n",
    "请在 task.py 文件中基于以下代码补充 #TODO 处的函数代码，确保能够实现目标。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
