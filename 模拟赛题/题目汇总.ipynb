{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention 与 KV 缓存\n",
    "https://www.lanqiao.cn/problems/20010/learning/?contest_id=251"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from typing import List, Optional, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, hidden_size: int = 512, num_attention_heads: int = 8) -> None:\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "\n",
    "\n",
    "class AttentionWithoutCache(nn.Module):\n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        assert self.head_dim * self.num_heads == self.hidden_size, \"hidden_size must be divisible by num_heads.\"\n",
    "\n",
    "        self.query_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.key_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.value_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        bsz, seq_len, _ = hidden_states.size()\n",
    "\n",
    "        query_states = self.query_proj(hidden_states).view(\n",
    "            bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = self.key_proj(hidden_states).view(\n",
    "            bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = self.value_proj(hidden_states).view(\n",
    "            bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn_weights = torch.matmul(\n",
    "            query_states, key_states.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, value_states).transpose(\n",
    "            1, 2).reshape(bsz, seq_len, self.hidden_size)\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "class AttentionWithCache(nn.Module):\n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        assert self.head_dim * \\\n",
    "            self.num_heads == self.hidden_size, \"hidden_size must be divisible by num_heads.\"\n",
    "\n",
    "        self.query_proj = nn.Linear(\n",
    "            self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.key_proj = nn.Linear(\n",
    "            self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.value_proj = nn.Linear(\n",
    "            self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, key_cache: Optional[torch.Tensor] = None, value_cache: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "        query_states = self.query_proj(hidden_states).view(\n",
    "            bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        if key_cache is not None:\n",
    "            key_states = torch.cat([key_cache, self.key_proj(hidden_states).view(\n",
    "                bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)], dim=2)\n",
    "        else:\n",
    "            key_states = self.key_proj(hidden_states).view(\n",
    "                bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        if value_cache is not None:\n",
    "            value_states = torch.cat([value_cache, self.value_proj(hidden_states).view(\n",
    "                bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)], dim=2)\n",
    "        else:\n",
    "            value_states = self.value_proj(hidden_states).view(\n",
    "                bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn_weights = torch.matmul(\n",
    "            query_states, key_states.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, value_states).transpose(\n",
    "            1, 2).reshape(bsz, q_len, self.hidden_size)\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        return attn_output, key_states, value_states\n",
    "\n",
    "\n",
    "def generate_input(bsz: int, seq_len: int, hidden_size: int) -> torch.Tensor:\n",
    "    \"\"\"生成随机的输入张量。\"\"\"\n",
    "    return torch.randn(bsz, seq_len, hidden_size)\n",
    "\n",
    "\n",
    "def test_without_cache(model: nn.Module, input_sequence: torch.Tensor, generation_length: int) -> List[torch.Tensor]:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_sequence = []\n",
    "\n",
    "        for step in range(generation_length):\n",
    "            output = model(input_sequence)[:,-1:,:]\n",
    "            generated_sequence.append(output)\n",
    "            input_sequence = torch.concat((input_sequence, output), dim=1)\n",
    "\n",
    "        return generated_sequence\n",
    "\n",
    "\n",
    "def test_with_cache(model: nn.Module, input_sequence: torch.Tensor, generation_length: int) -> List[torch.Tensor]:\n",
    "    # TODO\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_sequence = []\n",
    "        key_cache = None\n",
    "        value_cache = None\n",
    "        \n",
    "        for step in range(generation_length):\n",
    "            attn_output, key_cache, value_cache = model(input_sequence, key_cache, value_cache)\n",
    "            output = attn_output[:, -1:, :]  # Take the last token\n",
    "            generated_sequence.append(output)\n",
    "            input_sequence = output  # For next step, we only use the generated token\n",
    "            \n",
    "        return generated_sequence\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = Config(hidden_size=512, num_attention_heads=8)\n",
    "\n",
    "    bsz = 10\n",
    "    seq_len = 10\n",
    "    hidden_size = config.hidden_size\n",
    "    input_tensor = generate_input(bsz, 1, hidden_size).to(device)\n",
    "\n",
    "    # 使用缓存\n",
    "    attention_layer_with_cache = AttentionWithCache(config).to(device)\n",
    "    attention_layer_with_cache.load_state_dict(torch.load('model.pth'))\n",
    "    start_time = time.time()\n",
    "    with_cache_outputs = test_with_cache(attention_layer_with_cache, input_tensor, seq_len)\n",
    "    time_with_cache = time.time() - start_time\n",
    "    print(f\"Time with cache: {time_with_cache:.6f} seconds.\")\n",
    "\n",
    "    # 不使用缓存\n",
    "    attention_layer_without_cache = AttentionWithoutCache(config).to(device)\n",
    "    attention_layer_without_cache.load_state_dict(torch.load('model.pth'))\n",
    "    start_time = time.time()\n",
    "    without_cache_outputs = test_without_cache(attention_layer_without_cache, input_tensor, seq_len)\n",
    "    time_without_cache = time.time() - start_time\n",
    "    print(f\"Time without cache: {time_without_cache:.6f} seconds.\")\n",
    "\n",
    "    if time_with_cache < time_without_cache:\n",
    "        print(\"Using cache is faster.\")\n",
    "    else:\n",
    "        print(\"Using cache did not improve performance.\")\n",
    "\n",
    "    for idx, (without_cache_output, with_cache_output) in enumerate(zip(without_cache_outputs, with_cache_outputs)):\n",
    "        assert torch.allclose(with_cache_output, without_cache_output, atol=0.001)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图像分割推理与结果处理\n",
    "https://www.lanqiao.cn/problems/20009/learning/?contest_id=251"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "from collections import OrderedDict\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from cv2 import connectedComponentsWithStats\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet\n",
    "from torchvision.models.segmentation.fcn import FCN\n",
    "\n",
    "\n",
    "class FCNHead(nn.Sequential):\n",
    "    def __init__(self, in_channels: int, channels: int) -> None:\n",
    "        inter_channels = in_channels // 4\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(inter_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(inter_channels, channels, 1)\n",
    "        ]\n",
    "\n",
    "        super(FCNHead, self).__init__(*layers)\n",
    "\n",
    "\n",
    "class IntermediateLayerGetter(nn.ModuleDict):\n",
    "\n",
    "    def __init__(self, model: nn.Module, return_layers: Dict[str, str]) -> None:\n",
    "        if not set(return_layers).issubset([name for name, _ in model.named_children()]):\n",
    "            raise ValueError(\"return_layers are not present in model\")\n",
    "\n",
    "        orig_return_layers = return_layers\n",
    "        return_layers = {k: v for k, v in return_layers.items()}\n",
    "        layers = OrderedDict()\n",
    "        for name, module in model.named_children():\n",
    "            layers[name] = module\n",
    "            if name in return_layers:\n",
    "                del return_layers[name]\n",
    "            if not return_layers:\n",
    "                break\n",
    "\n",
    "        super(IntermediateLayerGetter, self).__init__(layers)\n",
    "        self.return_layers = orig_return_layers\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        out = OrderedDict()\n",
    "        for name, module in self.named_children():\n",
    "            x = module(x)\n",
    "            if name in self.return_layers:\n",
    "                out_name = self.return_layers[name]\n",
    "                out[out_name] = x\n",
    "        return out\n",
    "\n",
    "\n",
    "class FCNImageSegmenter(FCN):\n",
    "    def __init__(self, num_classes: int = 21, **kwargs: Any) -> None:\n",
    "        backbone = resnet.resnet101(\n",
    "            weights=None,\n",
    "            replace_stride_with_dilation=[False, True, True],\n",
    "        )\n",
    "        return_layers = {\"layer4\": \"out\"}\n",
    "        return_layers[\"layer3\"] = \"aux\"\n",
    "        backbone = IntermediateLayerGetter(\n",
    "            backbone, return_layers=return_layers)\n",
    "\n",
    "        inplanes = 1024\n",
    "        aux_classifier = FCNHead(inplanes, num_classes)\n",
    "        inplanes = 2048\n",
    "        classifier = FCNHead(inplanes, num_classes)\n",
    "\n",
    "        super(FCNImageSegmenter, self).__init__(\n",
    "            backbone, classifier, aux_classifier)\n",
    "\n",
    "\n",
    "def load_model(file_path: str) -> FCNImageSegmenter:\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "\n",
    "class ImageLoader:\n",
    "    def __init__(self, mean: List[float] = [0.485, 0.456, 0.406], std: List[float] = [0.229, 0.224, 0.225]) -> None:\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "def load_model(file_path: str) -> FCNImageSegmenter:\n",
    "    # TODO\n",
    "    model = FCNImageSegmenter()\n",
    "    model.load_state_dict(torch.load(file_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "class ImageLoader:\n",
    "    def __init__(self, mean: List[float] = [0.485, 0.456, 0.406], std: List[float] = [0.229, 0.224, 0.225]) -> None:\n",
    "        # TODO\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std)\n",
    "        ])\n",
    "    \n",
    "    def load_image(self, image_path: str) -> torch.Tensor:\n",
    "        # TODO\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        return image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "\n",
    "    def pred(model: nn.Module, input_batch: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO\n",
    "        with torch.no_grad():\n",
    "            output = model(input_batch)['out']\n",
    "        return torch.argmax(output.squeeze(0), dim=0)\n",
    "\n",
    "\n",
    "    def find_connected_components(prediction: np.ndarray, label_mapping: Dict[int, str], connectivity: int=8) -> Dict[str, List[List[List[int]]]]:\n",
    "        # TODO\n",
    "        result = {}\n",
    "        \n",
    "        for label_num, label_name in label_mapping.items():\n",
    "            mask = (prediction == label_num).astype(np.uint8)\n",
    "            num_labels, labels, stats, centroids = connectedComponentsWithStats(mask, connectivity=connectivity)\n",
    "            \n",
    "            components = []\n",
    "            for i in range(1, num_labels):  # Skip background (0)\n",
    "                # Get coordinates of all pixels in this component\n",
    "                y, x = np.where(labels == i)\n",
    "                points = list(zip(y.tolist(), x.tolist()))  # Convert to list of [y,x] pairs\n",
    "                components.append(points)\n",
    "            \n",
    "            if components:\n",
    "                result[label_name] = components\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    label_mapping = {0: 'Background', \n",
    "                     1: 'Aero plane',\n",
    "                     2: 'Bicycle',\n",
    "                     3: 'Bird',\n",
    "                     4: 'Boat',\n",
    "                     5: 'Bottle',\n",
    "                     6: 'Bus',\n",
    "                     7: 'Car',\n",
    "                     8: 'Cat',\n",
    "                     9: 'Chair',\n",
    "                     10: 'Cow',\n",
    "                     11: 'Dining table',\n",
    "                     12: 'Dog',\n",
    "                     13: 'Horse',\n",
    "                     14: 'Motorbike',\n",
    "                     15: 'Person',\n",
    "                     16: 'Potted plant',\n",
    "                     17: 'Sheep',\n",
    "                     18: 'Sofa',\n",
    "                     19: 'Train',\n",
    "                     20: 'Tv/Monitor'}\n",
    "    model = load_model('model.pth')\n",
    "    loader = ImageLoader()\n",
    "    input_batch = loader.load_image('test.jpg')\n",
    "    output = pred(model, input_batch)\n",
    "    components = find_connected_components(output.detach().numpy(), label_mapping)\n",
    "    \n",
    "    for label, class_points in components.items():\n",
    "        for i, points in enumerate(class_points, 1):\n",
    "            print(label, i, points[:10])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch 神经网络\n",
    "https://www.lanqiao.cn/problems/20008/learning/?contest_id=251"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import random\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def read_csv(file_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    #TODO\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    target = df['target']\n",
    "    featrues = df.drop(columns=['target'])\n",
    "\n",
    "    return featrues.values,target.values\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    #TODO\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(100, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64, 5)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def train_nn(X_train: torch.Tensor, y_train: torch.Tensor) -> Classifier:\n",
    "    model = Classifier()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    epochs = 150\n",
    "    for epoch in range(epochs):\n",
    "        #TODO\n",
    "        output = model(X_train)\n",
    "        loss = criterion(output, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model\n",
    "\n",
    "def save_model(model: nn.Module, file_path: str) -> None:\n",
    "    #TODO\n",
    "    torch.save(model.state_dict(), file_path)\n",
    "\n",
    "def main() -> None:\n",
    "\n",
    "    features, targets = read_csv('train_data.csv')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    model = train_nn(X_train_tensor, y_train_tensor)\n",
    "    save_model(model, 'model.pth')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提升 XGB 回归模型训练效果\n",
    "https://www.lanqiao.cn/problems/20007/learning/?contest_id=251"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import json\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def read_data(filename: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    #TODO\n",
    "    with open(filename,'r') as f:\n",
    "        data = json.load(f)\n",
    "        features = data['features']\n",
    "        target = data['target']\n",
    "        \n",
    "    features = np.array(features)\n",
    "    target = np.array(target)\n",
    "\n",
    "    return (features, target)\n",
    "\n",
    "def train(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray):\n",
    "    model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        learning_rate=0.1,\n",
    "        max_depth=4,\n",
    "        subsample=0.8,\n",
    "        reg_lambda=1.0,  # 修正参数名称\n",
    "        reg_alpha=0.0,   # 修正参数名称\n",
    "        eval_metric='rmse',  # 将评估指标移至此处\n",
    "        n_estimators=100\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        verbose=False  # 禁用训练日志输出\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def remove_least_important_feature(X_train: np.ndarray, X_test: np.ndarray, model: xgb.XGBRegressor) -> Tuple[int, np.ndarray, np.ndarray]:\n",
    "    # get feature_importances\n",
    "    importances = model.feature_importances_\n",
    "\n",
    "    # get least_important feature ids\n",
    "    least_important_ids = np.argmin(importances)\n",
    "\n",
    "    # 观察文件数据格式为numpy数组\n",
    "    X_train_removed = np.delete(X_train, least_important_ids, axis=1)\n",
    "    X_test_removed = np.delete(X_test, least_important_ids, axis=1)\n",
    "\n",
    "    return (least_important_ids, X_train_removed, X_test_removed)\n",
    "\n",
    "def main() -> None:\n",
    "    X, y = read_data('data.json')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = train(X_train, y_train, X_test, y_test)\n",
    "    least_important_index, X_train_reduced, X_test_reduced = remove_least_important_feature(X_train, X_test, model)\n",
    "    model_retrained = train(X_train_reduced, y_train, X_test_reduced, y_test)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义K-means距离\n",
    "https://www.lanqiao.cn/problems/20006/learning/?contest_id=251\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "from typing import Callable, Optional, Tuple\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "class CustomKMeans:\n",
    "    def __init__(self, \n",
    "                 n_clusters: int = 3, \n",
    "                 max_iter: int = 300, \n",
    "                 distance_metric: Callable[[np.ndarray, np.ndarray], np.ndarray] = None) -> None:\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        assert distance_metric is not None\n",
    "        self.distance_metric = distance_metric\n",
    "        self.cluster_centers_: Optional[np.ndarray] = None\n",
    "        self.labels_: Optional[np.ndarray] = None\n",
    "\n",
    "    def initialize_centroids(self, X: np.ndarray) -> np.ndarray:\n",
    "        indices = np.random.choice(X.shape[0], self.n_clusters, replace=False)\n",
    "        return X[indices]\n",
    "\n",
    "    def assign_clusters(self, X: np.ndarray, centroids: np.ndarray) -> np.ndarray:\n",
    "        distances = np.array([self.distance_metric(x, centroids) for x in X])\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        return labels\n",
    "\n",
    "    def compute_centroids(self, X: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "        centroids = np.zeros((self.n_clusters, X.shape[1]))\n",
    "        for i in range(self.n_clusters):\n",
    "            points_in_cluster = X[labels == i]\n",
    "            centroids[i] = np.mean(points_in_cluster, axis=0) if len(points_in_cluster) > 0 else self.cluster_centers_[i]\n",
    "        return centroids\n",
    "\n",
    "    def fit(self, X: np.ndarray, tol: float = 1e-6) -> 'CustomKMeans':\n",
    "\n",
    "        self.cluster_centers_ = self.initialize_centroids(X)\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            self.labels_ = self.assign_clusters(X, self.cluster_centers_)\n",
    "            new_centroids = self.compute_centroids(X, self.labels_)\n",
    "            if np.allclose(self.cluster_centers_, new_centroids, atol=tol):\n",
    "                break\n",
    "            self.cluster_centers_ = new_centroids\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        #TODO\n",
    "        return self.assign_clusters(X, self.cluster_centers_)\n",
    "\n",
    "\n",
    "def euclidean_distance(x1: np.ndarray, x2: np.ndarray) -> np.ndarray:\n",
    "    #TODO\n",
    "    return np.linalg.norm(x2-x1, axis=1,ord=2)\n",
    "def manhattan_distance(x1: np.ndarray, x2: np.ndarray) -> np.ndarray:\n",
    "    #TODO\n",
    "    return np.sum(np.abs(x2-x1),axis=1)\n",
    "def chebyshev_distance(x1: np.ndarray, x2: np.ndarray) -> np.ndarray:\n",
    "    #TODO\n",
    "    return np.max(np.abs(x2-x1),axis=1)\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    X = np.random.rand(100, 2)\n",
    "\n",
    "    kmeans_euclidean = CustomKMeans(n_clusters=3, distance_metric=euclidean_distance)\n",
    "    kmeans_euclidean.fit(X)\n",
    "    \n",
    "    kmeans_manhattan = CustomKMeans(n_clusters=3, distance_metric=manhattan_distance)\n",
    "    kmeans_manhattan.fit(X)\n",
    "    \n",
    "    kmeans_chebyshev = CustomKMeans(n_clusters=3, distance_metric=chebyshev_distance)\n",
    "    kmeans_chebyshev.fit(X)\n",
    "    \n",
    "    x = np.random.rand(3, 2)\n",
    "    euclidean_predict = kmeans_euclidean.predict(x)\n",
    "    manhattan_predict = kmeans_manhattan.predict(x)\n",
    "    chebyshev_predict = kmeans_chebyshev.predict(x)\n",
    "    print(f'{euclidean_predict=},{manhattan_predict=},{chebyshev_predict=}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于特征阈值划分数据集\n",
    "https://www.lanqiao.cn/problems/20005/learning/?contest_id=251"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def divide_on_feature(X, feature_i, threshold):\n",
    "    #TODO\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(-1,1)\n",
    "\n",
    "    x = X[:,feature_i]\n",
    "    \n",
    "    subset1 = X[x>=threshold]\n",
    "    subset2 = X[x<threshold]\n",
    "\n",
    "    return [subset1,subset2]\n",
    "\n",
    "def main():\n",
    "    # 示例数据集\n",
    "    X = np.array([[1, 2],\n",
    "                  [3, 4],\n",
    "                  [5, 6],\n",
    "                  [7, 8],\n",
    "                  [9, 10]])\n",
    "    feature_i = 0\n",
    "    threshold = 5\n",
    "    \n",
    "    # 执行数据集划分\n",
    "    subsets = divide_on_feature(X, feature_i, threshold)\n",
    "    \n",
    "    print(\"Subset 1:\")\n",
    "    print(subsets[0])\n",
    "    print(\"\\nSubset 2:\")\n",
    "    print(subsets[1])\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图像旋转\n",
    "https://www.lanqiao.cn/problems/20004/learning/?contest_id=251  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "import numpy as np\n",
    "\n",
    "def rotate_image(image, angle):\n",
    "    # 获取输入图像尺寸\n",
    "    height, width = image.shape\n",
    "    \n",
    "    # 将角度转换为弧度（顺时针旋转使用负角度）\n",
    "    theta = np.radians(-angle)\n",
    "    # 计算旋转后图像的边界框大小\n",
    "    cos_theta = np.abs(np.cos(theta))\n",
    "    sin_theta = np.abs(np.sin(theta))\n",
    "    new_width = int(np.ceil(width * cos_theta + height * sin_theta))\n",
    "    new_height = int(np.ceil(width * sin_theta + height * cos_theta))\n",
    "    # 对于特定角度，保持原始尺寸\n",
    "    if angle % 90 == 0:\n",
    "        new_width = width\n",
    "        new_height = height\n",
    "    \n",
    "    # 创建输出图像\n",
    "    output = np.zeros((new_height, new_width), dtype=np.uint8)\n",
    "    \n",
    "    # 计算输入和输出的中心点\n",
    "    center_x_in = (width - 1) / 2\n",
    "    center_y_in = (height - 1) / 2\n",
    "    center_x_out = (new_width - 1) / 2\n",
    "    center_y_out = (new_height - 1) / 2\n",
    "    \n",
    "    # 从输入图像的每个像素映射到输出位置\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            # 转换为以中心为原点的坐标\n",
    "            src_x = x - center_x_in\n",
    "            src_y = y - center_y_in     \n",
    "            # 应用旋转矩阵（顺时针）\n",
    "            dest_x = src_x * np.cos(theta) + src_y * np.sin(theta)\n",
    "            dest_y = -src_x * np.sin(theta) + src_y * np.cos(theta)\n",
    "            \n",
    "            # 转换到输出坐标系\n",
    "            dest_x = int(np.round(dest_x + center_x_out))\n",
    "            dest_y = int(np.round(dest_y + center_y_out))\n",
    "            \n",
    "            # 检查边界并赋值\n",
    "            if (0 <= dest_x < new_width) and (0 <= dest_y < new_height):\n",
    "                output[dest_y, dest_x] = image[y, x]\n",
    "    \n",
    "    return output\n",
    "def main():\n",
    "    image = np.array([[100, 150, 200, 50, 75],\n",
    "                      [80, 120, 160, 40, 60],\n",
    "                      [180, 210, 240, 90, 110],\n",
    "                      [20, 30, 40, 10, 15],\n",
    "                      [130, 170, 220, 70, 95]], dtype=np.uint8)\n",
    "    rotated_image = rotate_image(image, 90)\n",
    "    print(rotated_image)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
